{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T22:59:59.316602Z",
     "start_time": "2018-08-20T22:59:36.323475Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "feature_names=[\"Age\", \"Workclass\", \"Final Weight\", \"Education\", \"Education-Num\", \"Marital Status\",\n",
    "               \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n",
    "               \"Hours per week\", \"Country\", \"Income\"]\n",
    "\n",
    "df_train=pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\n",
    "                     names=feature_names)\n",
    "\n",
    "df_test=pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\",\n",
    "                    skiprows =1,\n",
    "                    names=feature_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep & problem definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T22:59:59.367604Z",
     "start_time": "2018-08-20T22:59:59.319605Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_ROWS=5000\n",
    "\n",
    "def df_prep(df):\n",
    "    df=df.copy().sample(SAMPLE_ROWS)\n",
    "    LABEL=\"Income\"\n",
    "    y_train=df[LABEL].replace([\" <=50K\",\" <=50K.\",\" >50K\",\" >50K.\"],[0,0,1,1])\n",
    "    df_X=df.drop(LABEL, axis=1)\n",
    "    return df_X,y_train\n",
    "\n",
    "df_X_train,y_train=df_prep(df_train)\n",
    "df_X_test,y_test=df_prep(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Very basic feature/classifier pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T22:59:59.444608Z",
     "start_time": "2018-08-20T22:59:59.371610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrameMapper(default=False, df_out=True,\n",
       "        features=[(['Age'], Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), (['Final Weight'], Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), (['Education-Num'], Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), (['C...sparse_output=False)), (['Country'], LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False))],\n",
       "        input_df=False, sparse=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn_pandas import gen_features\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "import sklearn.preprocessing as preprocessing\n",
    "\n",
    "nums=[ ([c],preprocessing.Imputer()) for c in df_X_train.select_dtypes([np.number])]\n",
    "cats=[ ([c],preprocessing.LabelBinarizer()) for c in df_X_train.select_dtypes([\"object\"])]\n",
    "\n",
    "feature_mapper=DataFrameMapper(nums+cats,df_out=True)\n",
    "feature_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T00:05:29.106681Z",
     "start_time": "2018-08-21T00:05:29.095672Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_dist = {\n",
    "              # Note n_estimators probably not a true hyperparameter, \n",
    "              # in general more is better (aside performance/diminishing returns)\n",
    "              \"classifier__n_estimators\": [10,20],\n",
    "              \"classifier__max_features\": ['auto', 'sqrt', 'log2'],\n",
    "              \"classifier__max_depth\": [1,8],\n",
    "              \"classifier__min_samples_leaf\": [1,8],\n",
    "              \"classifier__bootstrap\": [True,False],\n",
    "              \"classifier__n_jobs\":[-1],\n",
    "              \"classifier__criterion\" :['gini', 'entropy']\n",
    "             }\n",
    "\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "pipeline=Pipeline([('featurize', feature_mapper),\n",
    "                   ('classifier',classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T00:05:35.568710Z",
     "start_time": "2018-08-21T00:05:31.379717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n",
      "[CV] classifier__bootstrap=True, classifier__criterion=gini, classifier__max_depth=1, classifier__max_features=auto, classifier__min_samples_leaf=1, classifier__n_estimators=10, classifier__n_jobs=-1 \n",
      "[CV]  classifier__bootstrap=True, classifier__criterion=gini, classifier__max_depth=1, classifier__max_features=auto, classifier__min_samples_leaf=1, classifier__n_estimators=10, classifier__n_jobs=-1, score=0.8388277393581713, total=   0.4s\n",
      "[CV] classifier__bootstrap=True, classifier__criterion=gini, classifier__max_depth=1, classifier__max_features=auto, classifier__min_samples_leaf=1, classifier__n_estimators=10, classifier__n_jobs=-1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  classifier__bootstrap=True, classifier__criterion=gini, classifier__max_depth=1, classifier__max_features=auto, classifier__min_samples_leaf=1, classifier__n_estimators=10, classifier__n_jobs=-1, score=0.8245903584891239, total=   0.4s\n",
      "[CV] classifier__bootstrap=True, classifier__criterion=gini, classifier__max_depth=1, classifier__max_features=auto, classifier__min_samples_leaf=1, classifier__n_estimators=10, classifier__n_jobs=-1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    1.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  classifier__bootstrap=True, classifier__criterion=gini, classifier__max_depth=1, classifier__max_features=auto, classifier__min_samples_leaf=1, classifier__n_estimators=10, classifier__n_jobs=-1, score=0.8668694502096616, total=   0.4s\n",
      "[CV] classifier__bootstrap=True, classifier__criterion=gini, classifier__max_depth=1, classifier__max_features=auto, classifier__min_samples_leaf=1, classifier__n_estimators=20, classifier__n_jobs=-1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  classifier__bootstrap=True, classifier__criterion=gini, classifier__max_depth=1, classifier__max_features=auto, classifier__min_samples_leaf=1, classifier__n_estimators=20, classifier__n_jobs=-1, score=0.8589427165679022, total=   0.4s\n",
      "[CV] classifier__bootstrap=True, classifier__criterion=gini, classifier__max_depth=1, classifier__max_features=auto, classifier__min_samples_leaf=1, classifier__n_estimators=20, classifier__n_jobs=-1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    2.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  classifier__bootstrap=True, classifier__criterion=gini, classifier__max_depth=1, classifier__max_features=auto, classifier__min_samples_leaf=1, classifier__n_estimators=20, classifier__n_jobs=-1, score=0.8409679729332866, total=   0.4s\n",
      "[CV] classifier__bootstrap=True, classifier__criterion=gini, classifier__max_depth=1, classifier__max_features=auto, classifier__min_samples_leaf=1, classifier__n_estimators=20, classifier__n_jobs=-1 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, clf, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m                 \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, type)\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m                     \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattribute_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomForestClassifier' object has no attribute 'decision_function'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-bce71a17c8c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m                                    return_train_score=True)\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0msearch_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_X_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msearch_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    638\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    639\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 640\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    486\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[1;31m# _score will return dict if is_multimetric is True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[0mtest_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_score\u001b[1;34m(estimator, X_test, y_test, scorer, is_multimetric)\u001b[0m\n\u001b[0;32m    521\u001b[0m     \"\"\"\n\u001b[0;32m    522\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_multimetric_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_multimetric_score\u001b[1;34m(estimator, X_test, y_test, scorers)\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'item'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, clf, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mNotImplementedError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m                 \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'_final_estimator'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    587\u001b[0m         Parallel(n_jobs=n_jobs, verbose=self.verbose, backend=\"threading\")(\n\u001b[0;32m    588\u001b[0m             \u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccumulate_prediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_proba\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlock\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m             for e in self.estimators_)\n\u001b[0m\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mproba\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_proba\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    795\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 797\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_terminate_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    798\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_terminate_backend\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_terminate_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mterminate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# terminate does a join()\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mterminate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    539\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_worker_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_terminate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\multiprocessing\\util.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, wr, _finalizer_registry, sub_debug, getpid)\u001b[0m\n\u001b[0;32m    184\u001b[0m                 sub_debug('finalizer calling %s with args %s and kwargs %s',\n\u001b[0;32m    185\u001b[0m                           self._callback, self._args, self._kwargs)\n\u001b[1;32m--> 186\u001b[1;33m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_weakref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36m_terminate_pool\u001b[1;34m(cls, taskqueue, inqueue, outqueue, pool, worker_handler, task_handler, result_handler, cache)\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'joining worker handler'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mthreading\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_thread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mworker_handler\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m             \u001b[0mworker_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;31m# Terminate workers which haven't already finished.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m             \u001b[1;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1070\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# already determined that the C code is done\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1072\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Quick grid search\n",
    "CV=3\n",
    "\n",
    "search_cv = GridSearchCV(pipeline, param_grid=param_dist,\n",
    "                                   n_jobs=1,\n",
    "                                   scoring=\"roc_auc\",\n",
    "                                   error_score=0,cv=CV,verbose=5,\n",
    "                                   \n",
    "                                   #will not be default for sklearn .021\n",
    "                                   return_train_score=True)\n",
    "\n",
    "search_cv.fit(df_X_train,y_train)\n",
    "\n",
    "clf=search_cv.best_estimator_\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataframe for Dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:54:17.477829Z",
     "start_time": "2018-08-20T23:54:17.340830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier__bootstrap</th>\n",
       "      <th>param_classifier__criterion</th>\n",
       "      <th>param_classifier__max_depth</th>\n",
       "      <th>param_classifier__max_features</th>\n",
       "      <th>param_classifier__min_samples_leaf</th>\n",
       "      <th>param_classifier__n_estimators</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "      <th>dif_test_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.293997</td>\n",
       "      <td>0.019440</td>\n",
       "      <td>0.148001</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.902015</td>\n",
       "      <td>0.907172</td>\n",
       "      <td>0.006147</td>\n",
       "      <td>1</td>\n",
       "      <td>0.937665</td>\n",
       "      <td>0.939355</td>\n",
       "      <td>0.938127</td>\n",
       "      <td>0.938382</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.031211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.300667</td>\n",
       "      <td>0.012257</td>\n",
       "      <td>0.148336</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>8</td>\n",
       "      <td>auto</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.902015</td>\n",
       "      <td>0.907172</td>\n",
       "      <td>0.006147</td>\n",
       "      <td>1</td>\n",
       "      <td>0.937665</td>\n",
       "      <td>0.939355</td>\n",
       "      <td>0.938127</td>\n",
       "      <td>0.938382</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.031211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.296999</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.159004</td>\n",
       "      <td>0.002948</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>auto</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.903147</td>\n",
       "      <td>0.906732</td>\n",
       "      <td>0.005731</td>\n",
       "      <td>3</td>\n",
       "      <td>0.941294</td>\n",
       "      <td>0.943237</td>\n",
       "      <td>0.944918</td>\n",
       "      <td>0.943150</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.036417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.350668</td>\n",
       "      <td>0.026714</td>\n",
       "      <td>0.170667</td>\n",
       "      <td>0.012283</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.903147</td>\n",
       "      <td>0.906732</td>\n",
       "      <td>0.005731</td>\n",
       "      <td>3</td>\n",
       "      <td>0.941294</td>\n",
       "      <td>0.943237</td>\n",
       "      <td>0.944918</td>\n",
       "      <td>0.943150</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.036417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.301331</td>\n",
       "      <td>0.023213</td>\n",
       "      <td>0.162336</td>\n",
       "      <td>0.005251</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.901080</td>\n",
       "      <td>0.905514</td>\n",
       "      <td>0.006539</td>\n",
       "      <td>5</td>\n",
       "      <td>0.938398</td>\n",
       "      <td>0.942302</td>\n",
       "      <td>0.940659</td>\n",
       "      <td>0.940453</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.034939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.321664</td>\n",
       "      <td>0.015924</td>\n",
       "      <td>0.155339</td>\n",
       "      <td>0.003305</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>auto</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.901080</td>\n",
       "      <td>0.905514</td>\n",
       "      <td>0.006539</td>\n",
       "      <td>5</td>\n",
       "      <td>0.938398</td>\n",
       "      <td>0.942302</td>\n",
       "      <td>0.940659</td>\n",
       "      <td>0.940453</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.034939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.308335</td>\n",
       "      <td>0.003854</td>\n",
       "      <td>0.151999</td>\n",
       "      <td>0.002940</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.901801</td>\n",
       "      <td>0.904926</td>\n",
       "      <td>0.006580</td>\n",
       "      <td>7</td>\n",
       "      <td>0.922311</td>\n",
       "      <td>0.924506</td>\n",
       "      <td>0.926751</td>\n",
       "      <td>0.924523</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>0.019597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.309999</td>\n",
       "      <td>0.070412</td>\n",
       "      <td>0.148108</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>auto</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.901801</td>\n",
       "      <td>0.904926</td>\n",
       "      <td>0.006580</td>\n",
       "      <td>7</td>\n",
       "      <td>0.922311</td>\n",
       "      <td>0.924506</td>\n",
       "      <td>0.926751</td>\n",
       "      <td>0.924523</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>0.019597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.544331</td>\n",
       "      <td>0.062961</td>\n",
       "      <td>0.197004</td>\n",
       "      <td>0.025664</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.900718</td>\n",
       "      <td>0.904625</td>\n",
       "      <td>0.005922</td>\n",
       "      <td>9</td>\n",
       "      <td>0.937739</td>\n",
       "      <td>0.940967</td>\n",
       "      <td>0.940305</td>\n",
       "      <td>0.939670</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.035045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.364985</td>\n",
       "      <td>0.088467</td>\n",
       "      <td>0.150817</td>\n",
       "      <td>0.010380</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>8</td>\n",
       "      <td>auto</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.900718</td>\n",
       "      <td>0.904625</td>\n",
       "      <td>0.005922</td>\n",
       "      <td>9</td>\n",
       "      <td>0.937739</td>\n",
       "      <td>0.940967</td>\n",
       "      <td>0.940305</td>\n",
       "      <td>0.939670</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.035045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.272000</td>\n",
       "      <td>0.013879</td>\n",
       "      <td>0.152007</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>auto</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899589</td>\n",
       "      <td>0.904489</td>\n",
       "      <td>0.005335</td>\n",
       "      <td>11</td>\n",
       "      <td>0.937583</td>\n",
       "      <td>0.940486</td>\n",
       "      <td>0.940285</td>\n",
       "      <td>0.939452</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.034962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.298660</td>\n",
       "      <td>0.019599</td>\n",
       "      <td>0.150670</td>\n",
       "      <td>0.003679</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899589</td>\n",
       "      <td>0.904489</td>\n",
       "      <td>0.005335</td>\n",
       "      <td>11</td>\n",
       "      <td>0.937583</td>\n",
       "      <td>0.940486</td>\n",
       "      <td>0.940285</td>\n",
       "      <td>0.939452</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.034962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.269996</td>\n",
       "      <td>0.008526</td>\n",
       "      <td>0.151001</td>\n",
       "      <td>0.002827</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899429</td>\n",
       "      <td>0.904399</td>\n",
       "      <td>0.007027</td>\n",
       "      <td>13</td>\n",
       "      <td>0.936398</td>\n",
       "      <td>0.937329</td>\n",
       "      <td>0.936880</td>\n",
       "      <td>0.936869</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.032470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.271333</td>\n",
       "      <td>0.009428</td>\n",
       "      <td>0.151339</td>\n",
       "      <td>0.004924</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>8</td>\n",
       "      <td>auto</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899429</td>\n",
       "      <td>0.904399</td>\n",
       "      <td>0.007027</td>\n",
       "      <td>13</td>\n",
       "      <td>0.936398</td>\n",
       "      <td>0.937329</td>\n",
       "      <td>0.936880</td>\n",
       "      <td>0.936869</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.032470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.291998</td>\n",
       "      <td>0.027531</td>\n",
       "      <td>0.150669</td>\n",
       "      <td>0.002622</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>auto</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899853</td>\n",
       "      <td>0.904251</td>\n",
       "      <td>0.005124</td>\n",
       "      <td>15</td>\n",
       "      <td>0.940419</td>\n",
       "      <td>0.940672</td>\n",
       "      <td>0.943972</td>\n",
       "      <td>0.941688</td>\n",
       "      <td>0.001619</td>\n",
       "      <td>0.037436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.289666</td>\n",
       "      <td>0.017326</td>\n",
       "      <td>0.162670</td>\n",
       "      <td>0.018661</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899853</td>\n",
       "      <td>0.904251</td>\n",
       "      <td>0.005124</td>\n",
       "      <td>15</td>\n",
       "      <td>0.940419</td>\n",
       "      <td>0.940672</td>\n",
       "      <td>0.943972</td>\n",
       "      <td>0.941688</td>\n",
       "      <td>0.001619</td>\n",
       "      <td>0.037436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.316001</td>\n",
       "      <td>0.008981</td>\n",
       "      <td>0.155003</td>\n",
       "      <td>0.004548</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899207</td>\n",
       "      <td>0.904205</td>\n",
       "      <td>0.007147</td>\n",
       "      <td>17</td>\n",
       "      <td>0.918260</td>\n",
       "      <td>0.922706</td>\n",
       "      <td>0.921181</td>\n",
       "      <td>0.920716</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>0.016511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.310997</td>\n",
       "      <td>0.025665</td>\n",
       "      <td>0.153001</td>\n",
       "      <td>0.002828</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>auto</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899207</td>\n",
       "      <td>0.904205</td>\n",
       "      <td>0.007147</td>\n",
       "      <td>17</td>\n",
       "      <td>0.918260</td>\n",
       "      <td>0.922706</td>\n",
       "      <td>0.921181</td>\n",
       "      <td>0.920716</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>0.016511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.259973</td>\n",
       "      <td>0.022615</td>\n",
       "      <td>0.141773</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>8</td>\n",
       "      <td>log2</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.903034</td>\n",
       "      <td>0.903871</td>\n",
       "      <td>0.006273</td>\n",
       "      <td>19</td>\n",
       "      <td>0.925955</td>\n",
       "      <td>0.931898</td>\n",
       "      <td>0.934391</td>\n",
       "      <td>0.930748</td>\n",
       "      <td>0.003539</td>\n",
       "      <td>0.026877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.275669</td>\n",
       "      <td>0.015523</td>\n",
       "      <td>0.147337</td>\n",
       "      <td>0.004193</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>auto</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899141</td>\n",
       "      <td>0.903851</td>\n",
       "      <td>0.008704</td>\n",
       "      <td>20</td>\n",
       "      <td>0.917932</td>\n",
       "      <td>0.921983</td>\n",
       "      <td>0.921165</td>\n",
       "      <td>0.920360</td>\n",
       "      <td>0.001749</td>\n",
       "      <td>0.016509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.300331</td>\n",
       "      <td>0.060664</td>\n",
       "      <td>0.147336</td>\n",
       "      <td>0.005559</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899141</td>\n",
       "      <td>0.903851</td>\n",
       "      <td>0.008704</td>\n",
       "      <td>20</td>\n",
       "      <td>0.917932</td>\n",
       "      <td>0.921983</td>\n",
       "      <td>0.921165</td>\n",
       "      <td>0.920360</td>\n",
       "      <td>0.001749</td>\n",
       "      <td>0.016509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.257999</td>\n",
       "      <td>0.006531</td>\n",
       "      <td>0.161004</td>\n",
       "      <td>0.016755</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>auto</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.903398</td>\n",
       "      <td>0.903636</td>\n",
       "      <td>0.005244</td>\n",
       "      <td>22</td>\n",
       "      <td>0.919481</td>\n",
       "      <td>0.923441</td>\n",
       "      <td>0.926201</td>\n",
       "      <td>0.923041</td>\n",
       "      <td>0.002758</td>\n",
       "      <td>0.019405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.310665</td>\n",
       "      <td>0.014728</td>\n",
       "      <td>0.166336</td>\n",
       "      <td>0.005250</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.903398</td>\n",
       "      <td>0.903636</td>\n",
       "      <td>0.005244</td>\n",
       "      <td>22</td>\n",
       "      <td>0.919481</td>\n",
       "      <td>0.923441</td>\n",
       "      <td>0.926201</td>\n",
       "      <td>0.923041</td>\n",
       "      <td>0.002758</td>\n",
       "      <td>0.019405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0.465997</td>\n",
       "      <td>0.119768</td>\n",
       "      <td>0.159340</td>\n",
       "      <td>0.004111</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>8</td>\n",
       "      <td>log2</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.902463</td>\n",
       "      <td>0.903485</td>\n",
       "      <td>0.007676</td>\n",
       "      <td>24</td>\n",
       "      <td>0.932417</td>\n",
       "      <td>0.932862</td>\n",
       "      <td>0.936874</td>\n",
       "      <td>0.934051</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>0.030566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.306326</td>\n",
       "      <td>0.010340</td>\n",
       "      <td>0.160001</td>\n",
       "      <td>0.006531</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>log2</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.898595</td>\n",
       "      <td>0.903394</td>\n",
       "      <td>0.005393</td>\n",
       "      <td>25</td>\n",
       "      <td>0.932672</td>\n",
       "      <td>0.937910</td>\n",
       "      <td>0.937986</td>\n",
       "      <td>0.936189</td>\n",
       "      <td>0.002487</td>\n",
       "      <td>0.032795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.317666</td>\n",
       "      <td>0.021817</td>\n",
       "      <td>0.155335</td>\n",
       "      <td>0.004782</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>log2</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.898419</td>\n",
       "      <td>0.903368</td>\n",
       "      <td>0.007647</td>\n",
       "      <td>26</td>\n",
       "      <td>0.930473</td>\n",
       "      <td>0.933303</td>\n",
       "      <td>0.935558</td>\n",
       "      <td>0.933112</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>0.029743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.649998</td>\n",
       "      <td>0.299214</td>\n",
       "      <td>0.268336</td>\n",
       "      <td>0.097038</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>8</td>\n",
       "      <td>auto</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.900596</td>\n",
       "      <td>0.903345</td>\n",
       "      <td>0.006063</td>\n",
       "      <td>27</td>\n",
       "      <td>0.918802</td>\n",
       "      <td>0.924584</td>\n",
       "      <td>0.924800</td>\n",
       "      <td>0.922729</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.019383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0.647333</td>\n",
       "      <td>0.132056</td>\n",
       "      <td>0.200669</td>\n",
       "      <td>0.035034</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.900596</td>\n",
       "      <td>0.903345</td>\n",
       "      <td>0.006063</td>\n",
       "      <td>27</td>\n",
       "      <td>0.918802</td>\n",
       "      <td>0.924584</td>\n",
       "      <td>0.924800</td>\n",
       "      <td>0.922729</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.019383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.289331</td>\n",
       "      <td>0.023615</td>\n",
       "      <td>0.156005</td>\n",
       "      <td>0.006684</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>log2</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.900156</td>\n",
       "      <td>0.903073</td>\n",
       "      <td>0.005805</td>\n",
       "      <td>29</td>\n",
       "      <td>0.931707</td>\n",
       "      <td>0.934809</td>\n",
       "      <td>0.937585</td>\n",
       "      <td>0.934701</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>0.031628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.310999</td>\n",
       "      <td>0.008983</td>\n",
       "      <td>0.152336</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>8</td>\n",
       "      <td>auto</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899827</td>\n",
       "      <td>0.902838</td>\n",
       "      <td>0.006143</td>\n",
       "      <td>30</td>\n",
       "      <td>0.915247</td>\n",
       "      <td>0.922290</td>\n",
       "      <td>0.919768</td>\n",
       "      <td>0.919102</td>\n",
       "      <td>0.002914</td>\n",
       "      <td>0.016263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.272665</td>\n",
       "      <td>0.020825</td>\n",
       "      <td>0.144997</td>\n",
       "      <td>0.006685</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.852655</td>\n",
       "      <td>0.842952</td>\n",
       "      <td>0.006887</td>\n",
       "      <td>185</td>\n",
       "      <td>0.822126</td>\n",
       "      <td>0.858596</td>\n",
       "      <td>0.874656</td>\n",
       "      <td>0.851793</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>0.008841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.230991</td>\n",
       "      <td>0.004544</td>\n",
       "      <td>0.146398</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.852655</td>\n",
       "      <td>0.842952</td>\n",
       "      <td>0.006887</td>\n",
       "      <td>185</td>\n",
       "      <td>0.822126</td>\n",
       "      <td>0.858596</td>\n",
       "      <td>0.874656</td>\n",
       "      <td>0.851793</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>0.008841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.272001</td>\n",
       "      <td>0.005353</td>\n",
       "      <td>0.155666</td>\n",
       "      <td>0.003770</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867251</td>\n",
       "      <td>0.841918</td>\n",
       "      <td>0.018650</td>\n",
       "      <td>189</td>\n",
       "      <td>0.811308</td>\n",
       "      <td>0.839043</td>\n",
       "      <td>0.876022</td>\n",
       "      <td>0.842124</td>\n",
       "      <td>0.026509</td>\n",
       "      <td>0.000206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.216997</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.152346</td>\n",
       "      <td>0.004643</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>1</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867251</td>\n",
       "      <td>0.841918</td>\n",
       "      <td>0.018650</td>\n",
       "      <td>189</td>\n",
       "      <td>0.811308</td>\n",
       "      <td>0.839043</td>\n",
       "      <td>0.876022</td>\n",
       "      <td>0.842124</td>\n",
       "      <td>0.026509</td>\n",
       "      <td>0.000206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.257998</td>\n",
       "      <td>0.010615</td>\n",
       "      <td>0.151004</td>\n",
       "      <td>0.006170</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867251</td>\n",
       "      <td>0.841918</td>\n",
       "      <td>0.018650</td>\n",
       "      <td>189</td>\n",
       "      <td>0.811308</td>\n",
       "      <td>0.839043</td>\n",
       "      <td>0.876022</td>\n",
       "      <td>0.842124</td>\n",
       "      <td>0.026509</td>\n",
       "      <td>0.000206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.237677</td>\n",
       "      <td>0.023466</td>\n",
       "      <td>0.142346</td>\n",
       "      <td>0.001703</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>1</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867251</td>\n",
       "      <td>0.841918</td>\n",
       "      <td>0.018650</td>\n",
       "      <td>189</td>\n",
       "      <td>0.811308</td>\n",
       "      <td>0.839043</td>\n",
       "      <td>0.876022</td>\n",
       "      <td>0.842124</td>\n",
       "      <td>0.026509</td>\n",
       "      <td>0.000206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.228837</td>\n",
       "      <td>0.013293</td>\n",
       "      <td>0.143949</td>\n",
       "      <td>0.001905</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.850227</td>\n",
       "      <td>0.838898</td>\n",
       "      <td>0.009584</td>\n",
       "      <td>193</td>\n",
       "      <td>0.817966</td>\n",
       "      <td>0.853060</td>\n",
       "      <td>0.867346</td>\n",
       "      <td>0.846124</td>\n",
       "      <td>0.020747</td>\n",
       "      <td>0.007226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.255329</td>\n",
       "      <td>0.050143</td>\n",
       "      <td>0.144412</td>\n",
       "      <td>0.004745</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.850227</td>\n",
       "      <td>0.838898</td>\n",
       "      <td>0.009584</td>\n",
       "      <td>193</td>\n",
       "      <td>0.817966</td>\n",
       "      <td>0.853060</td>\n",
       "      <td>0.867346</td>\n",
       "      <td>0.846124</td>\n",
       "      <td>0.020747</td>\n",
       "      <td>0.007226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.215328</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>0.149091</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.850227</td>\n",
       "      <td>0.838898</td>\n",
       "      <td>0.009584</td>\n",
       "      <td>193</td>\n",
       "      <td>0.817966</td>\n",
       "      <td>0.853060</td>\n",
       "      <td>0.867346</td>\n",
       "      <td>0.846124</td>\n",
       "      <td>0.020747</td>\n",
       "      <td>0.007226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.236714</td>\n",
       "      <td>0.005389</td>\n",
       "      <td>0.155772</td>\n",
       "      <td>0.010392</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.850227</td>\n",
       "      <td>0.838898</td>\n",
       "      <td>0.009584</td>\n",
       "      <td>193</td>\n",
       "      <td>0.817966</td>\n",
       "      <td>0.853060</td>\n",
       "      <td>0.867346</td>\n",
       "      <td>0.846124</td>\n",
       "      <td>0.020747</td>\n",
       "      <td>0.007226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.228788</td>\n",
       "      <td>0.005069</td>\n",
       "      <td>0.155875</td>\n",
       "      <td>0.011404</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>2</td>\n",
       "      <td>log2</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.840069</td>\n",
       "      <td>0.836604</td>\n",
       "      <td>0.013622</td>\n",
       "      <td>197</td>\n",
       "      <td>0.844513</td>\n",
       "      <td>0.838859</td>\n",
       "      <td>0.851212</td>\n",
       "      <td>0.844861</td>\n",
       "      <td>0.005049</td>\n",
       "      <td>0.008257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.234709</td>\n",
       "      <td>0.016769</td>\n",
       "      <td>0.140898</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>2</td>\n",
       "      <td>log2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.838910</td>\n",
       "      <td>0.836583</td>\n",
       "      <td>0.013873</td>\n",
       "      <td>198</td>\n",
       "      <td>0.845475</td>\n",
       "      <td>0.839010</td>\n",
       "      <td>0.851580</td>\n",
       "      <td>0.845355</td>\n",
       "      <td>0.005133</td>\n",
       "      <td>0.008772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.254334</td>\n",
       "      <td>0.025103</td>\n",
       "      <td>0.149336</td>\n",
       "      <td>0.004187</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.849926</td>\n",
       "      <td>0.835572</td>\n",
       "      <td>0.011682</td>\n",
       "      <td>199</td>\n",
       "      <td>0.812107</td>\n",
       "      <td>0.854215</td>\n",
       "      <td>0.866488</td>\n",
       "      <td>0.844270</td>\n",
       "      <td>0.023288</td>\n",
       "      <td>0.008698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.220989</td>\n",
       "      <td>0.007062</td>\n",
       "      <td>0.154200</td>\n",
       "      <td>0.006977</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.849926</td>\n",
       "      <td>0.835572</td>\n",
       "      <td>0.011682</td>\n",
       "      <td>199</td>\n",
       "      <td>0.812107</td>\n",
       "      <td>0.854215</td>\n",
       "      <td>0.866488</td>\n",
       "      <td>0.844270</td>\n",
       "      <td>0.023288</td>\n",
       "      <td>0.008698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.225095</td>\n",
       "      <td>0.007007</td>\n",
       "      <td>0.146343</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>1</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.849926</td>\n",
       "      <td>0.835572</td>\n",
       "      <td>0.011682</td>\n",
       "      <td>199</td>\n",
       "      <td>0.812107</td>\n",
       "      <td>0.854215</td>\n",
       "      <td>0.866488</td>\n",
       "      <td>0.844270</td>\n",
       "      <td>0.023288</td>\n",
       "      <td>0.008698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.243455</td>\n",
       "      <td>0.025743</td>\n",
       "      <td>0.157119</td>\n",
       "      <td>0.004255</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>1</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.849926</td>\n",
       "      <td>0.835572</td>\n",
       "      <td>0.011682</td>\n",
       "      <td>199</td>\n",
       "      <td>0.812107</td>\n",
       "      <td>0.854215</td>\n",
       "      <td>0.866488</td>\n",
       "      <td>0.844270</td>\n",
       "      <td>0.023288</td>\n",
       "      <td>0.008698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.288001</td>\n",
       "      <td>0.021926</td>\n",
       "      <td>0.164666</td>\n",
       "      <td>0.009177</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>2</td>\n",
       "      <td>log2</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.837396</td>\n",
       "      <td>0.834473</td>\n",
       "      <td>0.007570</td>\n",
       "      <td>203</td>\n",
       "      <td>0.836038</td>\n",
       "      <td>0.842826</td>\n",
       "      <td>0.847903</td>\n",
       "      <td>0.842256</td>\n",
       "      <td>0.004861</td>\n",
       "      <td>0.007782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.283001</td>\n",
       "      <td>0.034031</td>\n",
       "      <td>0.148671</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>2</td>\n",
       "      <td>log2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.836677</td>\n",
       "      <td>0.834385</td>\n",
       "      <td>0.007489</td>\n",
       "      <td>204</td>\n",
       "      <td>0.836641</td>\n",
       "      <td>0.843026</td>\n",
       "      <td>0.848590</td>\n",
       "      <td>0.842752</td>\n",
       "      <td>0.004882</td>\n",
       "      <td>0.008367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.269995</td>\n",
       "      <td>0.014302</td>\n",
       "      <td>0.148669</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>2</td>\n",
       "      <td>log2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.835449</td>\n",
       "      <td>0.832849</td>\n",
       "      <td>0.018569</td>\n",
       "      <td>205</td>\n",
       "      <td>0.845511</td>\n",
       "      <td>0.826468</td>\n",
       "      <td>0.848747</td>\n",
       "      <td>0.840242</td>\n",
       "      <td>0.009829</td>\n",
       "      <td>0.007393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.259993</td>\n",
       "      <td>0.004550</td>\n",
       "      <td>0.151668</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>2</td>\n",
       "      <td>log2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.837135</td>\n",
       "      <td>0.832733</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>206</td>\n",
       "      <td>0.841211</td>\n",
       "      <td>0.826395</td>\n",
       "      <td>0.848416</td>\n",
       "      <td>0.838674</td>\n",
       "      <td>0.009167</td>\n",
       "      <td>0.005940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.263999</td>\n",
       "      <td>0.008637</td>\n",
       "      <td>0.151677</td>\n",
       "      <td>0.005323</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>2</td>\n",
       "      <td>log2</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.837047</td>\n",
       "      <td>0.831830</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>207</td>\n",
       "      <td>0.840103</td>\n",
       "      <td>0.823386</td>\n",
       "      <td>0.848591</td>\n",
       "      <td>0.837360</td>\n",
       "      <td>0.010471</td>\n",
       "      <td>0.005530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.280334</td>\n",
       "      <td>0.020170</td>\n",
       "      <td>0.156664</td>\n",
       "      <td>0.005245</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>2</td>\n",
       "      <td>log2</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.834984</td>\n",
       "      <td>0.831101</td>\n",
       "      <td>0.019829</td>\n",
       "      <td>208</td>\n",
       "      <td>0.844685</td>\n",
       "      <td>0.821653</td>\n",
       "      <td>0.848381</td>\n",
       "      <td>0.838240</td>\n",
       "      <td>0.011825</td>\n",
       "      <td>0.007139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.473332</td>\n",
       "      <td>0.180551</td>\n",
       "      <td>0.235669</td>\n",
       "      <td>0.090761</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>log2</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816073</td>\n",
       "      <td>0.818184</td>\n",
       "      <td>0.017177</td>\n",
       "      <td>209</td>\n",
       "      <td>0.827207</td>\n",
       "      <td>0.818475</td>\n",
       "      <td>0.833780</td>\n",
       "      <td>0.826487</td>\n",
       "      <td>0.006269</td>\n",
       "      <td>0.008303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.223986</td>\n",
       "      <td>0.007477</td>\n",
       "      <td>0.147573</td>\n",
       "      <td>0.001942</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>log2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816073</td>\n",
       "      <td>0.818184</td>\n",
       "      <td>0.017177</td>\n",
       "      <td>209</td>\n",
       "      <td>0.827207</td>\n",
       "      <td>0.818475</td>\n",
       "      <td>0.833780</td>\n",
       "      <td>0.826487</td>\n",
       "      <td>0.006269</td>\n",
       "      <td>0.008303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.243022</td>\n",
       "      <td>0.016238</td>\n",
       "      <td>0.145508</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>1</td>\n",
       "      <td>log2</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816047</td>\n",
       "      <td>0.815872</td>\n",
       "      <td>0.020485</td>\n",
       "      <td>211</td>\n",
       "      <td>0.828693</td>\n",
       "      <td>0.815474</td>\n",
       "      <td>0.834706</td>\n",
       "      <td>0.826291</td>\n",
       "      <td>0.008033</td>\n",
       "      <td>0.010419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.221297</td>\n",
       "      <td>0.009981</td>\n",
       "      <td>0.144376</td>\n",
       "      <td>0.002070</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>1</td>\n",
       "      <td>log2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816047</td>\n",
       "      <td>0.815872</td>\n",
       "      <td>0.020485</td>\n",
       "      <td>211</td>\n",
       "      <td>0.828693</td>\n",
       "      <td>0.815474</td>\n",
       "      <td>0.834706</td>\n",
       "      <td>0.826291</td>\n",
       "      <td>0.008033</td>\n",
       "      <td>0.010419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.231306</td>\n",
       "      <td>0.010786</td>\n",
       "      <td>0.146777</td>\n",
       "      <td>0.005863</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>1</td>\n",
       "      <td>log2</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.822736</td>\n",
       "      <td>0.802851</td>\n",
       "      <td>0.039475</td>\n",
       "      <td>213</td>\n",
       "      <td>0.818042</td>\n",
       "      <td>0.771447</td>\n",
       "      <td>0.841457</td>\n",
       "      <td>0.810315</td>\n",
       "      <td>0.029099</td>\n",
       "      <td>0.007464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.255321</td>\n",
       "      <td>0.034700</td>\n",
       "      <td>0.151096</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>1</td>\n",
       "      <td>log2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.822736</td>\n",
       "      <td>0.802851</td>\n",
       "      <td>0.039475</td>\n",
       "      <td>213</td>\n",
       "      <td>0.818042</td>\n",
       "      <td>0.771447</td>\n",
       "      <td>0.841457</td>\n",
       "      <td>0.810315</td>\n",
       "      <td>0.029099</td>\n",
       "      <td>0.007464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.268331</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.153670</td>\n",
       "      <td>0.006801</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>log2</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.818144</td>\n",
       "      <td>0.792448</td>\n",
       "      <td>0.047741</td>\n",
       "      <td>215</td>\n",
       "      <td>0.818348</td>\n",
       "      <td>0.745345</td>\n",
       "      <td>0.839111</td>\n",
       "      <td>0.800934</td>\n",
       "      <td>0.040212</td>\n",
       "      <td>0.008487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.325664</td>\n",
       "      <td>0.092995</td>\n",
       "      <td>0.218671</td>\n",
       "      <td>0.057817</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>log2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.818144</td>\n",
       "      <td>0.792448</td>\n",
       "      <td>0.047741</td>\n",
       "      <td>215</td>\n",
       "      <td>0.818348</td>\n",
       "      <td>0.745345</td>\n",
       "      <td>0.839111</td>\n",
       "      <td>0.800934</td>\n",
       "      <td>0.040212</td>\n",
       "      <td>0.008487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216 rows  23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "98        0.293997      0.019440         0.148001        0.003559   \n",
       "92        0.300667      0.012257         0.148336        0.001248   \n",
       "146       0.296999      0.009200         0.159004        0.002948   \n",
       "152       0.350668      0.026714         0.170667        0.012283   \n",
       "44        0.301331      0.023213         0.162336        0.005251   \n",
       "38        0.321664      0.015924         0.155339        0.003305   \n",
       "155       0.308335      0.003854         0.151999        0.002940   \n",
       "149       0.309999      0.070412         0.148108        0.000944   \n",
       "206       0.544331      0.062961         0.197004        0.025664   \n",
       "200       0.364985      0.088467         0.150817        0.010380   \n",
       "37        0.272000      0.013879         0.152007        0.002154   \n",
       "43        0.298660      0.019599         0.150670        0.003679   \n",
       "97        0.269996      0.008526         0.151001        0.002827   \n",
       "91        0.271333      0.009428         0.151339        0.004924   \n",
       "145       0.291998      0.027531         0.150669        0.002622   \n",
       "151       0.289666      0.017326         0.162670        0.018661   \n",
       "47        0.316001      0.008981         0.155003        0.004548   \n",
       "41        0.310997      0.025665         0.153001        0.002828   \n",
       "104       0.259973      0.022615         0.141773        0.001080   \n",
       "40        0.275669      0.015523         0.147337        0.004193   \n",
       "46        0.300331      0.060664         0.147336        0.005559   \n",
       "148       0.257999      0.006531         0.161004        0.016755   \n",
       "154       0.310665      0.014728         0.166336        0.005250   \n",
       "212       0.465997      0.119768         0.159340        0.004111   \n",
       "158       0.306326      0.010340         0.160001        0.006531   \n",
       "50        0.317666      0.021817         0.155335        0.004782   \n",
       "203       0.649998      0.299214         0.268336        0.097038   \n",
       "209       0.647333      0.132056         0.200669        0.035034   \n",
       "157       0.289331      0.023615         0.156005        0.006684   \n",
       "95        0.310999      0.008983         0.152336        0.001246   \n",
       "..             ...           ...              ...             ...   \n",
       "166       0.272665      0.020825         0.144997        0.006685   \n",
       "163       0.230991      0.004544         0.146398        0.001713   \n",
       "54        0.272001      0.005353         0.155666        0.003770   \n",
       "63        0.216997      0.002942         0.152346        0.004643   \n",
       "57        0.257998      0.010615         0.151004        0.006170   \n",
       "60        0.237677      0.023466         0.142346        0.001703   \n",
       "108       0.228837      0.013293         0.143949        0.001905   \n",
       "111       0.255329      0.050143         0.144412        0.004745   \n",
       "114       0.215328      0.001693         0.149091        0.000828   \n",
       "117       0.236714      0.005389         0.155772        0.010392   \n",
       "195       0.228788      0.005069         0.155875        0.011404   \n",
       "192       0.234709      0.016769         0.140898        0.000446   \n",
       "162       0.254334      0.025103         0.149336        0.004187   \n",
       "165       0.220989      0.007062         0.154200        0.006977   \n",
       "168       0.225095      0.007007         0.146343        0.001244   \n",
       "171       0.243455      0.025743         0.157119        0.004255   \n",
       "141       0.288001      0.021926         0.164666        0.009177   \n",
       "138       0.283001      0.034031         0.148671        0.001698   \n",
       "30        0.269995      0.014302         0.148669        0.004500   \n",
       "84        0.259993      0.004550         0.151668        0.000944   \n",
       "87        0.263999      0.008637         0.151677        0.005323   \n",
       "33        0.280334      0.020170         0.156664        0.005245   \n",
       "123       0.473332      0.180551         0.235669        0.090761   \n",
       "120       0.223986      0.007477         0.147573        0.001942   \n",
       "177       0.243022      0.016238         0.145508        0.005421   \n",
       "174       0.221297      0.009981         0.144376        0.002070   \n",
       "69        0.231306      0.010786         0.146777        0.005863   \n",
       "66        0.255321      0.034700         0.151096        0.000821   \n",
       "15        0.268331      0.006944         0.153670        0.006801   \n",
       "12        0.325664      0.092995         0.218671        0.057817   \n",
       "\n",
       "    param_classifier__bootstrap param_classifier__criterion  \\\n",
       "98                         True                     entropy   \n",
       "92                         True                     entropy   \n",
       "146                       False                        gini   \n",
       "152                       False                        gini   \n",
       "44                         True                        gini   \n",
       "38                         True                        gini   \n",
       "155                       False                        gini   \n",
       "149                       False                        gini   \n",
       "206                       False                     entropy   \n",
       "200                       False                     entropy   \n",
       "37                         True                        gini   \n",
       "43                         True                        gini   \n",
       "97                         True                     entropy   \n",
       "91                         True                     entropy   \n",
       "145                       False                        gini   \n",
       "151                       False                        gini   \n",
       "47                         True                        gini   \n",
       "41                         True                        gini   \n",
       "104                        True                     entropy   \n",
       "40                         True                        gini   \n",
       "46                         True                        gini   \n",
       "148                       False                        gini   \n",
       "154                       False                        gini   \n",
       "212                       False                     entropy   \n",
       "158                       False                        gini   \n",
       "50                         True                        gini   \n",
       "203                       False                     entropy   \n",
       "209                       False                     entropy   \n",
       "157                       False                        gini   \n",
       "95                         True                     entropy   \n",
       "..                          ...                         ...   \n",
       "166                       False                     entropy   \n",
       "163                       False                     entropy   \n",
       "54                         True                     entropy   \n",
       "63                         True                     entropy   \n",
       "57                         True                     entropy   \n",
       "60                         True                     entropy   \n",
       "108                       False                        gini   \n",
       "111                       False                        gini   \n",
       "114                       False                        gini   \n",
       "117                       False                        gini   \n",
       "195                       False                     entropy   \n",
       "192                       False                     entropy   \n",
       "162                       False                     entropy   \n",
       "165                       False                     entropy   \n",
       "168                       False                     entropy   \n",
       "171                       False                     entropy   \n",
       "141                       False                        gini   \n",
       "138                       False                        gini   \n",
       "30                         True                        gini   \n",
       "84                         True                     entropy   \n",
       "87                         True                     entropy   \n",
       "33                         True                        gini   \n",
       "123                       False                        gini   \n",
       "120                       False                        gini   \n",
       "177                       False                     entropy   \n",
       "174                       False                     entropy   \n",
       "69                         True                     entropy   \n",
       "66                         True                     entropy   \n",
       "15                         True                        gini   \n",
       "12                         True                        gini   \n",
       "\n",
       "    param_classifier__max_depth param_classifier__max_features  \\\n",
       "98                            8                           sqrt   \n",
       "92                            8                           auto   \n",
       "146                           8                           auto   \n",
       "152                           8                           sqrt   \n",
       "44                            8                           sqrt   \n",
       "38                            8                           auto   \n",
       "155                           8                           sqrt   \n",
       "149                           8                           auto   \n",
       "206                           8                           sqrt   \n",
       "200                           8                           auto   \n",
       "37                            8                           auto   \n",
       "43                            8                           sqrt   \n",
       "97                            8                           sqrt   \n",
       "91                            8                           auto   \n",
       "145                           8                           auto   \n",
       "151                           8                           sqrt   \n",
       "47                            8                           sqrt   \n",
       "41                            8                           auto   \n",
       "104                           8                           log2   \n",
       "40                            8                           auto   \n",
       "46                            8                           sqrt   \n",
       "148                           8                           auto   \n",
       "154                           8                           sqrt   \n",
       "212                           8                           log2   \n",
       "158                           8                           log2   \n",
       "50                            8                           log2   \n",
       "203                           8                           auto   \n",
       "209                           8                           sqrt   \n",
       "157                           8                           log2   \n",
       "95                            8                           auto   \n",
       "..                          ...                            ...   \n",
       "166                           1                           auto   \n",
       "163                           1                           auto   \n",
       "54                            1                           auto   \n",
       "63                            1                           sqrt   \n",
       "57                            1                           auto   \n",
       "60                            1                           sqrt   \n",
       "108                           1                           auto   \n",
       "111                           1                           auto   \n",
       "114                           1                           sqrt   \n",
       "117                           1                           sqrt   \n",
       "195                           2                           log2   \n",
       "192                           2                           log2   \n",
       "162                           1                           auto   \n",
       "165                           1                           auto   \n",
       "168                           1                           sqrt   \n",
       "171                           1                           sqrt   \n",
       "141                           2                           log2   \n",
       "138                           2                           log2   \n",
       "30                            2                           log2   \n",
       "84                            2                           log2   \n",
       "87                            2                           log2   \n",
       "33                            2                           log2   \n",
       "123                           1                           log2   \n",
       "120                           1                           log2   \n",
       "177                           1                           log2   \n",
       "174                           1                           log2   \n",
       "69                            1                           log2   \n",
       "66                            1                           log2   \n",
       "15                            1                           log2   \n",
       "12                            1                           log2   \n",
       "\n",
       "    param_classifier__min_samples_leaf param_classifier__n_estimators  \\\n",
       "98                                   1                             40   \n",
       "92                                   1                             40   \n",
       "146                                  1                             40   \n",
       "152                                  1                             40   \n",
       "44                                   1                             40   \n",
       "38                                   1                             40   \n",
       "155                                  8                             40   \n",
       "149                                  8                             40   \n",
       "206                                  1                             40   \n",
       "200                                  1                             40   \n",
       "37                                   1                             20   \n",
       "43                                   1                             20   \n",
       "97                                   1                             20   \n",
       "91                                   1                             20   \n",
       "145                                  1                             20   \n",
       "151                                  1                             20   \n",
       "47                                   8                             40   \n",
       "41                                   8                             40   \n",
       "104                                  1                             40   \n",
       "40                                   8                             20   \n",
       "46                                   8                             20   \n",
       "148                                  8                             20   \n",
       "154                                  8                             20   \n",
       "212                                  1                             40   \n",
       "158                                  1                             40   \n",
       "50                                   1                             40   \n",
       "203                                  8                             40   \n",
       "209                                  8                             40   \n",
       "157                                  1                             20   \n",
       "95                                   8                             40   \n",
       "..                                 ...                            ...   \n",
       "166                                  8                             20   \n",
       "163                                  1                             20   \n",
       "54                                   1                             10   \n",
       "63                                   8                             10   \n",
       "57                                   8                             10   \n",
       "60                                   1                             10   \n",
       "108                                  1                             10   \n",
       "111                                  8                             10   \n",
       "114                                  1                             10   \n",
       "117                                  8                             10   \n",
       "195                                  8                             10   \n",
       "192                                  1                             10   \n",
       "162                                  1                             10   \n",
       "165                                  8                             10   \n",
       "168                                  1                             10   \n",
       "171                                  8                             10   \n",
       "141                                  8                             10   \n",
       "138                                  1                             10   \n",
       "30                                   1                             10   \n",
       "84                                   1                             10   \n",
       "87                                   8                             10   \n",
       "33                                   8                             10   \n",
       "123                                  8                             10   \n",
       "120                                  1                             10   \n",
       "177                                  8                             10   \n",
       "174                                  1                             10   \n",
       "69                                   8                             10   \n",
       "66                                   1                             10   \n",
       "15                                   8                             10   \n",
       "12                                   1                             10   \n",
       "\n",
       "          ...       split2_test_score  mean_test_score  std_test_score  \\\n",
       "98        ...                0.902015         0.907172        0.006147   \n",
       "92        ...                0.902015         0.907172        0.006147   \n",
       "146       ...                0.903147         0.906732        0.005731   \n",
       "152       ...                0.903147         0.906732        0.005731   \n",
       "44        ...                0.901080         0.905514        0.006539   \n",
       "38        ...                0.901080         0.905514        0.006539   \n",
       "155       ...                0.901801         0.904926        0.006580   \n",
       "149       ...                0.901801         0.904926        0.006580   \n",
       "206       ...                0.900718         0.904625        0.005922   \n",
       "200       ...                0.900718         0.904625        0.005922   \n",
       "37        ...                0.899589         0.904489        0.005335   \n",
       "43        ...                0.899589         0.904489        0.005335   \n",
       "97        ...                0.899429         0.904399        0.007027   \n",
       "91        ...                0.899429         0.904399        0.007027   \n",
       "145       ...                0.899853         0.904251        0.005124   \n",
       "151       ...                0.899853         0.904251        0.005124   \n",
       "47        ...                0.899207         0.904205        0.007147   \n",
       "41        ...                0.899207         0.904205        0.007147   \n",
       "104       ...                0.903034         0.903871        0.006273   \n",
       "40        ...                0.899141         0.903851        0.008704   \n",
       "46        ...                0.899141         0.903851        0.008704   \n",
       "148       ...                0.903398         0.903636        0.005244   \n",
       "154       ...                0.903398         0.903636        0.005244   \n",
       "212       ...                0.902463         0.903485        0.007676   \n",
       "158       ...                0.898595         0.903394        0.005393   \n",
       "50        ...                0.898419         0.903368        0.007647   \n",
       "203       ...                0.900596         0.903345        0.006063   \n",
       "209       ...                0.900596         0.903345        0.006063   \n",
       "157       ...                0.900156         0.903073        0.005805   \n",
       "95        ...                0.899827         0.902838        0.006143   \n",
       "..        ...                     ...              ...             ...   \n",
       "166       ...                0.852655         0.842952        0.006887   \n",
       "163       ...                0.852655         0.842952        0.006887   \n",
       "54        ...                0.867251         0.841918        0.018650   \n",
       "63        ...                0.867251         0.841918        0.018650   \n",
       "57        ...                0.867251         0.841918        0.018650   \n",
       "60        ...                0.867251         0.841918        0.018650   \n",
       "108       ...                0.850227         0.838898        0.009584   \n",
       "111       ...                0.850227         0.838898        0.009584   \n",
       "114       ...                0.850227         0.838898        0.009584   \n",
       "117       ...                0.850227         0.838898        0.009584   \n",
       "195       ...                0.840069         0.836604        0.013622   \n",
       "192       ...                0.838910         0.836583        0.013873   \n",
       "162       ...                0.849926         0.835572        0.011682   \n",
       "165       ...                0.849926         0.835572        0.011682   \n",
       "168       ...                0.849926         0.835572        0.011682   \n",
       "171       ...                0.849926         0.835572        0.011682   \n",
       "141       ...                0.837396         0.834473        0.007570   \n",
       "138       ...                0.836677         0.834385        0.007489   \n",
       "30        ...                0.835449         0.832849        0.018569   \n",
       "84        ...                0.837135         0.832733        0.019481   \n",
       "87        ...                0.837047         0.831830        0.020450   \n",
       "33        ...                0.834984         0.831101        0.019829   \n",
       "123       ...                0.816073         0.818184        0.017177   \n",
       "120       ...                0.816073         0.818184        0.017177   \n",
       "177       ...                0.816047         0.815872        0.020485   \n",
       "174       ...                0.816047         0.815872        0.020485   \n",
       "69        ...                0.822736         0.802851        0.039475   \n",
       "66        ...                0.822736         0.802851        0.039475   \n",
       "15        ...                0.818144         0.792448        0.047741   \n",
       "12        ...                0.818144         0.792448        0.047741   \n",
       "\n",
       "     rank_test_score  split0_train_score  split1_train_score  \\\n",
       "98                 1            0.937665            0.939355   \n",
       "92                 1            0.937665            0.939355   \n",
       "146                3            0.941294            0.943237   \n",
       "152                3            0.941294            0.943237   \n",
       "44                 5            0.938398            0.942302   \n",
       "38                 5            0.938398            0.942302   \n",
       "155                7            0.922311            0.924506   \n",
       "149                7            0.922311            0.924506   \n",
       "206                9            0.937739            0.940967   \n",
       "200                9            0.937739            0.940967   \n",
       "37                11            0.937583            0.940486   \n",
       "43                11            0.937583            0.940486   \n",
       "97                13            0.936398            0.937329   \n",
       "91                13            0.936398            0.937329   \n",
       "145               15            0.940419            0.940672   \n",
       "151               15            0.940419            0.940672   \n",
       "47                17            0.918260            0.922706   \n",
       "41                17            0.918260            0.922706   \n",
       "104               19            0.925955            0.931898   \n",
       "40                20            0.917932            0.921983   \n",
       "46                20            0.917932            0.921983   \n",
       "148               22            0.919481            0.923441   \n",
       "154               22            0.919481            0.923441   \n",
       "212               24            0.932417            0.932862   \n",
       "158               25            0.932672            0.937910   \n",
       "50                26            0.930473            0.933303   \n",
       "203               27            0.918802            0.924584   \n",
       "209               27            0.918802            0.924584   \n",
       "157               29            0.931707            0.934809   \n",
       "95                30            0.915247            0.922290   \n",
       "..               ...                 ...                 ...   \n",
       "166              185            0.822126            0.858596   \n",
       "163              185            0.822126            0.858596   \n",
       "54               189            0.811308            0.839043   \n",
       "63               189            0.811308            0.839043   \n",
       "57               189            0.811308            0.839043   \n",
       "60               189            0.811308            0.839043   \n",
       "108              193            0.817966            0.853060   \n",
       "111              193            0.817966            0.853060   \n",
       "114              193            0.817966            0.853060   \n",
       "117              193            0.817966            0.853060   \n",
       "195              197            0.844513            0.838859   \n",
       "192              198            0.845475            0.839010   \n",
       "162              199            0.812107            0.854215   \n",
       "165              199            0.812107            0.854215   \n",
       "168              199            0.812107            0.854215   \n",
       "171              199            0.812107            0.854215   \n",
       "141              203            0.836038            0.842826   \n",
       "138              204            0.836641            0.843026   \n",
       "30               205            0.845511            0.826468   \n",
       "84               206            0.841211            0.826395   \n",
       "87               207            0.840103            0.823386   \n",
       "33               208            0.844685            0.821653   \n",
       "123              209            0.827207            0.818475   \n",
       "120              209            0.827207            0.818475   \n",
       "177              211            0.828693            0.815474   \n",
       "174              211            0.828693            0.815474   \n",
       "69               213            0.818042            0.771447   \n",
       "66               213            0.818042            0.771447   \n",
       "15               215            0.818348            0.745345   \n",
       "12               215            0.818348            0.745345   \n",
       "\n",
       "     split2_train_score  mean_train_score  std_train_score  dif_test_train  \n",
       "98             0.938127          0.938382         0.000713        0.031211  \n",
       "92             0.938127          0.938382         0.000713        0.031211  \n",
       "146            0.944918          0.943150         0.001481        0.036417  \n",
       "152            0.944918          0.943150         0.001481        0.036417  \n",
       "44             0.940659          0.940453         0.001601        0.034939  \n",
       "38             0.940659          0.940453         0.001601        0.034939  \n",
       "155            0.926751          0.924523         0.001813        0.019597  \n",
       "149            0.926751          0.924523         0.001813        0.019597  \n",
       "206            0.940305          0.939670         0.001392        0.035045  \n",
       "200            0.940305          0.939670         0.001392        0.035045  \n",
       "37             0.940285          0.939452         0.001324        0.034962  \n",
       "43             0.940285          0.939452         0.001324        0.034962  \n",
       "97             0.936880          0.936869         0.000380        0.032470  \n",
       "91             0.936880          0.936869         0.000380        0.032470  \n",
       "145            0.943972          0.941688         0.001619        0.037436  \n",
       "151            0.943972          0.941688         0.001619        0.037436  \n",
       "47             0.921181          0.920716         0.001845        0.016511  \n",
       "41             0.921181          0.920716         0.001845        0.016511  \n",
       "104            0.934391          0.930748         0.003539        0.026877  \n",
       "40             0.921165          0.920360         0.001749        0.016509  \n",
       "46             0.921165          0.920360         0.001749        0.016509  \n",
       "148            0.926201          0.923041         0.002758        0.019405  \n",
       "154            0.926201          0.923041         0.002758        0.019405  \n",
       "212            0.936874          0.934051         0.002004        0.030566  \n",
       "158            0.937986          0.936189         0.002487        0.032795  \n",
       "50             0.935558          0.933112         0.002080        0.029743  \n",
       "203            0.924800          0.922729         0.002778        0.019383  \n",
       "209            0.924800          0.922729         0.002778        0.019383  \n",
       "157            0.937585          0.934701         0.002401        0.031628  \n",
       "95             0.919768          0.919102         0.002914        0.016263  \n",
       "..                  ...               ...              ...             ...  \n",
       "166            0.874656          0.851793         0.021978        0.008841  \n",
       "163            0.874656          0.851793         0.021978        0.008841  \n",
       "54             0.876022          0.842124         0.026509        0.000206  \n",
       "63             0.876022          0.842124         0.026509        0.000206  \n",
       "57             0.876022          0.842124         0.026509        0.000206  \n",
       "60             0.876022          0.842124         0.026509        0.000206  \n",
       "108            0.867346          0.846124         0.020747        0.007226  \n",
       "111            0.867346          0.846124         0.020747        0.007226  \n",
       "114            0.867346          0.846124         0.020747        0.007226  \n",
       "117            0.867346          0.846124         0.020747        0.007226  \n",
       "195            0.851212          0.844861         0.005049        0.008257  \n",
       "192            0.851580          0.845355         0.005133        0.008772  \n",
       "162            0.866488          0.844270         0.023288        0.008698  \n",
       "165            0.866488          0.844270         0.023288        0.008698  \n",
       "168            0.866488          0.844270         0.023288        0.008698  \n",
       "171            0.866488          0.844270         0.023288        0.008698  \n",
       "141            0.847903          0.842256         0.004861        0.007782  \n",
       "138            0.848590          0.842752         0.004882        0.008367  \n",
       "30             0.848747          0.840242         0.009829        0.007393  \n",
       "84             0.848416          0.838674         0.009167        0.005940  \n",
       "87             0.848591          0.837360         0.010471        0.005530  \n",
       "33             0.848381          0.838240         0.011825        0.007139  \n",
       "123            0.833780          0.826487         0.006269        0.008303  \n",
       "120            0.833780          0.826487         0.006269        0.008303  \n",
       "177            0.834706          0.826291         0.008033        0.010419  \n",
       "174            0.834706          0.826291         0.008033        0.010419  \n",
       "69             0.841457          0.810315         0.029099        0.007464  \n",
       "66             0.841457          0.810315         0.029099        0.007464  \n",
       "15             0.839111          0.800934         0.040212        0.008487  \n",
       "12             0.839111          0.800934         0.040212        0.008487  \n",
       "\n",
       "[216 rows x 23 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results_df=pd.DataFrame(search_cv.cv_results_).sort_values(by='rank_test_score')\n",
    "cv_results_df[\"dif_test_train\"]=cv_results_df.mean_train_score-cv_results_df.mean_test_score\n",
    "\n",
    "# drop param list column, doesnt work in dash, not needed for now\n",
    "cv_results_df.drop(\"params\",axis=1,inplace=True)\n",
    "\n",
    "cv_results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dash "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:54:18.796309Z",
     "start_time": "2018-08-20T23:54:18.785305Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# From awesome dash intro repo by Kevin Mader\n",
    "# A quick intro to Dash made for the PyData event in Zurich\n",
    "# https://github.com/4QuantOSS/DashIntro \n",
    "\n",
    "# Can use Jupyter nbserverproxy extension (available at /.../proxy/<port>)\n",
    "\n",
    "def show_app(app, port = 10001, \n",
    "             width = 700, \n",
    "             height = 350, \n",
    "             offline = False,\n",
    "            in_binder = None):\n",
    "    in_binder ='JUPYTERHUB_SERVICE_PREFIX' in os.environ if in_binder is None else in_binder\n",
    "    if in_binder:\n",
    "        base_prefix = '{}proxy/{}/'.format(os.environ['JUPYTERHUB_SERVICE_PREFIX'], port)\n",
    "        url = 'https://hub.mybinder.org{}'.format(base_prefix)\n",
    "        app.config.requests_pathname_prefix = base_prefix\n",
    "    else:\n",
    "        url = 'http://localhost:%d' % port\n",
    "    iframe = '<a href=\"{url}\" target=\"_new\">Open in new window</a><hr><iframe src=\"{url}\" width={width} height={height}></iframe>'.format(url = url, \n",
    "                                                                                  width = width, \n",
    "                                                                                  height = height)\n",
    "\n",
    "    iframe = '<a href=\"{url}\" target=\"_new\">Open in new window</a><hr>'.format(url = url, \n",
    "                                                                                  width = width, \n",
    "                                                                                  height = height)\n",
    "\n",
    "    display.display_html(iframe, raw = True)\n",
    "    if offline:\n",
    "        app.css.config.serve_locally = True\n",
    "        app.scripts.config.serve_locally = True\n",
    "        \n",
    "    return app.run_server(debug=False, # needs to be false in Jupyter\n",
    "                          host = '0.0.0.0',\n",
    "                          port=port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:54:19.886962Z",
     "start_time": "2018-08-20T23:54:19.474581Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tools\\Anaconda3\\lib\\site-packages\\seaborn\\categorical.py:598: FutureWarning:\n",
      "\n",
      "remove_na is deprecated and is a private function. Do not use.\n",
      "\n",
      "C:\\tools\\Anaconda3\\lib\\site-packages\\seaborn\\categorical.py:826: FutureWarning:\n",
      "\n",
      "remove_na is deprecated and is a private function. Do not use.\n",
      "\n",
      "C:\\Users\\rquintino\\AppData\\Roaming\\Python\\Python36\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning:\n",
      "\n",
      "Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "\n",
      "C:\\tools\\Anaconda3\\lib\\site-packages\\seaborn\\categorical.py:1460: FutureWarning:\n",
      "\n",
      "remove_na is deprecated and is a private function. Do not use.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiQAAAF3CAYAAACVAVC0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEhEREYmcAomIiIhE7v8DjbTycIk8RbMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "import plotly.graph_objs as go\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def serve_param_figure(df,col,metric_1,metric_2):\n",
    "  \n",
    "    split_col=\"\"\n",
    "    fig=plt.figure(figsize=(8,6))\n",
    "    sns.violinplot(x=col, y=metric_1,data=df)\n",
    "    ax2 = plt.twinx()\n",
    "    sns.pointplot(x=col, y=metric_2,ax=ax2, data=df)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Test\n",
    "df=cv_results_df\n",
    "metric_1=\"mean_test_score\"\n",
    "metric_2=\"mean_fit_time\"\n",
    "col=\"param_classifier__max_depth\"\n",
    "\n",
    "fig=serve_param_figure(cv_results_df,col,metric_1,metric_2)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:55:50.421136Z",
     "start_time": "2018-08-20T23:55:50.303137Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tools\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning:\n",
      "\n",
      "convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\n",
      "For all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Coefficients</th>\n",
       "      <th>Standard Errors</th>\n",
       "      <th>Probabilites</th>\n",
       "      <th>AbsCoef</th>\n",
       "      <th>p?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>intercept/default</td>\n",
       "      <td>0.8346</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.8346</td>\n",
       "      <td>***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>param_classifier__max_features_log2</td>\n",
       "      <td>-0.0100</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>param_classifier__max_depth</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>param_classifier__n_estimators</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>param_classifier__criterion_gini</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.0019</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>param_classifier__bootstrap_True</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.0010</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>param_classifier__min_samples_leaf</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.0001</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>param_classifier__max_features_sqrt</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Column  Coefficients  Standard Errors  \\\n",
       "0                    intercept/default        0.8346            0.003   \n",
       "6  param_classifier__max_features_log2       -0.0100            0.002   \n",
       "1          param_classifier__max_depth        0.0068            0.000   \n",
       "3       param_classifier__n_estimators        0.0007            0.000   \n",
       "5     param_classifier__criterion_gini        0.0019            0.001   \n",
       "4     param_classifier__bootstrap_True        0.0010            0.001   \n",
       "2   param_classifier__min_samples_leaf       -0.0001            0.000   \n",
       "7  param_classifier__max_features_sqrt        0.0000            0.002   \n",
       "\n",
       "   Probabilites  AbsCoef   p?  \n",
       "0         0.000   0.8346  ***  \n",
       "6         0.000   0.0100  ***  \n",
       "1         0.000   0.0068  ***  \n",
       "3         0.000   0.0007  ***  \n",
       "5         0.203   0.0019       \n",
       "4         0.507   0.0010       \n",
       "2         0.522   0.0001       \n",
       "7         1.000   0.0000       "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.linear_model as linear_model\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from IPython.display import display as display\n",
    "\n",
    "# Needs review... use builtin instead\n",
    "def serve_regression_stats(df,score_result):\n",
    "\n",
    "    def drop_constant_columns(dataframe):\n",
    "        return dataframe.loc[:, (dataframe != dataframe.iloc[0]).any()]\n",
    "\n",
    "    all_reg=drop_constant_column(df)\n",
    "\n",
    "    reg_cols=[]\n",
    "    cat_cols=[]\n",
    "    for col in all_reg.columns:\n",
    "        if  col.startswith(\"param_\") or col==score_result:\n",
    "            reg_cols.append(col)\n",
    "\n",
    "    all_reg=all_reg[reg_cols]\n",
    "    all_reg=all_reg.convert_objects(convert_numeric=True)\n",
    "    all_reg\n",
    "\n",
    "    cat_cols=all_reg.select_dtypes(include=['object','category','bool']).columns\n",
    "    cat_cols\n",
    "    all_reg=pd.get_dummies(all_reg, columns=cat_cols,drop_first=True).fillna(0)\n",
    "    all_reg\n",
    "\n",
    "    y=all_reg[score_result].values\n",
    "    X=all_reg.drop(score_result,axis=1)\n",
    "    X_df=X\n",
    "\n",
    "    X = pd.DataFrame(X, columns = X_df.columns)\n",
    "    \n",
    "    lm = linear_model.LinearRegression(fit_intercept=True, normalize=True)\n",
    "    lm.fit(X,y)\n",
    "    params = np.append(lm.intercept_,lm.coef_)\n",
    "    predictions = lm.predict(X)\n",
    "\n",
    "    # https://stackoverflow.com/questions/27928275/find-p-value-significance-in-scikit-learn-linearregression/46912457\n",
    "    newX = pd.DataFrame({\"Constant\":np.ones(len(X))}).join(pd.DataFrame(X))\n",
    "    MSE = (sum((y-predictions)**2))/(len(newX)-len(newX.columns))\n",
    "\n",
    "    var_b = MSE*(np.linalg.inv(np.dot(newX.T,newX)).diagonal())\n",
    "    sd_b = np.sqrt(var_b)\n",
    "    ts_b = params/ sd_b\n",
    "\n",
    "    p_values =[2*(1-stats.t.cdf(np.abs(i),(len(newX)-1))) for i in ts_b]\n",
    "\n",
    "    sd_b = np.round(sd_b,3)\n",
    "    ts_b = np.round(ts_b,3)\n",
    "    p_values = np.round(p_values,3)\n",
    "    params = np.round(params,4)\n",
    "\n",
    "    results_df = pd.DataFrame()\n",
    "    results_df[\"Column\"]=X_df.columns.insert(0,\"intercept/default\")\n",
    "    results_df[\"Coefficients\"],results_df[\"Standard Errors\"],results_df[\"t values\"],results_df[\"Probabilites\"] = [params,sd_b,ts_b,p_values]\n",
    "\n",
    "    results_df[\"AbsCoef\"]=abs(results_df[\"Coefficients\"])\n",
    "\n",
    "    results_df[\"p?\"]=np.where(results_df['Probabilites']<.005, '***', np.where(results_df['Probabilites']<.05, '*', ''))\n",
    "\n",
    "    results_df.sort_values([\"p?\",\"AbsCoef\"],ascending=False,inplace=True)\n",
    "    \n",
    "    results_df.drop(\"t values\",axis=1,inplace=True)\n",
    "    return results_df\n",
    "\n",
    "# Test\n",
    "serve_regression_stats(cv_results_df,\"mean_test_score\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:56:12.001329Z",
     "start_time": "2018-08-20T23:56:11.889333Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import dash\n",
    "from dash.dependencies import Input, Output, State,Event\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import dash_table_experiments as dt\n",
    "import json\n",
    "import plotly\n",
    "from IPython import display\n",
    "from matplotlib import rcParams\n",
    "\n",
    "app = dash.Dash()\n",
    "\n",
    "metrics_list=cv_results_df.columns.values\n",
    "\n",
    "def fig_to_uri(in_fig, close_all=True, **save_args):\n",
    "    rcParams.update({'figure.autolayout': False})\n",
    "    out_img = BytesIO()\n",
    "    # cut issue: https://stackoverflow.com/questions/29901422/matplotlib-with-annotation-cut-off-from-the-saved-figure/29901470\n",
    "    in_fig.savefig(out_img, format='png',bbox_inches=\"tight\", **save_args)\n",
    "    if close_all:\n",
    "        in_fig.clf()\n",
    "        plt.close('all')\n",
    "    out_img.seek(0)  # rewind file\n",
    "    encoded = base64.b64encode(out_img.read()).decode(\"ascii\").replace(\"\\n\", \"\")\n",
    "    return \"data:image/png;base64,{}\".format(encoded)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    # Reference: https://github.com/plotly/dash-svm/\n",
    "    # .container class is fixed, .container.scalable is scalable\n",
    "    html.Div(className=\"banner\", children=[\n",
    "        html.Div(className='container scalable', children=[\n",
    "            html.H2(html.A(\n",
    "                'Dash Search CV Eval - DevScope AI Lab',\n",
    "                href='https://github.com/DevScope/ai-lab',\n",
    "                style={\n",
    "                    'text-decoration': 'none',\n",
    "                    'color': 'inherit'\n",
    "                }\n",
    "            )),\n",
    "\n",
    "            html.A(\n",
    "                html.Img(src=\"https://s3-us-west-1.amazonaws.com/plotly-tutorials/logo/new-branding/dash-logo-by-plotly-stripe-inverted.png\"),\n",
    "                href='https://plot.ly/products/dash/'\n",
    "            )\n",
    "        ]),\n",
    "    ]),\n",
    "    html.Div(id='body', className='container scalable', children=[\n",
    "         html.Div([\n",
    "            html.Div(\n",
    "                [\n",
    "                    dt.DataTable(\n",
    "                        rows=cv_results_df.to_dict('records'),\n",
    "                        editable=False,\n",
    "                        sortable=True,\n",
    "                        columns=cv_results_df.columns.values,\n",
    "                        row_selectable=False,\n",
    "                        filterable=True,\n",
    "                        max_rows_in_viewport=4,\n",
    "                        id='score_table'\n",
    "                        ),\n",
    "                    \n",
    "                    dt.DataTable(\n",
    "                        rows=[{'No Rows': ''}],\n",
    "                        editable=False,\n",
    "                        sortable=True,\n",
    "                        #columns=cv_results_df.columns.values,\n",
    "                        row_selectable=False,\n",
    "                        filterable=True,\n",
    "                        max_rows_in_viewport=4,\n",
    "                        id='regression_table'\n",
    "                        ),\n",
    "                ],className=\"six columns\"),\n",
    "             html.Div(\n",
    "                 [\n",
    "                     html.Div(className=\"row\",children=[\n",
    "                         dcc.Dropdown(\n",
    "                             id='metric_1',\n",
    "                             options=[{'label':label,'value':label} for label in metrics_list],\n",
    "                             value=\"mean_test_score\",\n",
    "                             placeholder=\"Main axis\",\n",
    "                             className=\"six columns\"\n",
    "                            ),\n",
    "                        dcc.Dropdown(\n",
    "                             id='metric_2',\n",
    "                             options=[{'label':label,'value':label} for label in metrics_list],\n",
    "                             value=\"mean_fit_time\",\n",
    "                             placeholder=\"Secondary axis\",\n",
    "                            className=\"six columns\"\n",
    "                            )]),\n",
    "                    html.Div(id=\"output\",style={'height':'500px','overflow-y': 'scroll'})\n",
    "                   ],id=\"results\",className=\"six columns\")\n",
    "        ],className=\"row\")\n",
    "     ])\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "   Output('regression_table', 'rows'),\n",
    "   [Input(\"score_table\",\"rows\"),Input(\"metric_1\",\"value\"),Input(\"metric_2\",\"value\")])\n",
    "def update_regression_table(rows,metric_1,metric_2):\n",
    "    if len(rows)==0:\n",
    "        return []\n",
    "    \n",
    "    children=[]\n",
    "    \n",
    "    df_selected = pd.DataFrame(rows)\n",
    "    \n",
    "    df_regression= serve_regression_stats(df_selected,metric_1)\n",
    "    \n",
    "    return df_regression.to_dict('records')\n",
    "    \n",
    "@app.callback(\n",
    "   Output('output', 'children'),\n",
    "   [Input(\"score_table\",\"rows\"),Input(\"metric_1\",\"value\"),Input(\"metric_2\",\"value\")])\n",
    "def update_output_table(rows,metric_1,metric_2):\n",
    "    if len(rows)==0:\n",
    "        return []\n",
    "    \n",
    "    children=[]\n",
    "    \n",
    "    #rebuild dataframe from received data\n",
    "    df_selected = pd.DataFrame(rows)\n",
    "    \n",
    "    for col in df_selected.columns:\n",
    "        if col.startswith(\"param_\") and len(df_selected[col].unique())>1:\n",
    "            col_figure = serve_param_figure(df_selected,col,metric_1,metric_2)\n",
    "            children.append(html.Img(src = fig_to_uri(col_figure)))\n",
    "    \n",
    "    return children\n",
    "\n",
    "# Reference: https://github.com/plotly/dash-svm/\n",
    "external_css = [\n",
    "    # Normalize the CSS\n",
    "    \"https://cdnjs.cloudflare.com/ajax/libs/normalize/7.0.0/normalize.min.css\",\n",
    "    # Fonts\n",
    "    \"https://fonts.googleapis.com/css?family=Open+Sans|Roboto\",\n",
    "    \"https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css\",\n",
    "    # Base Stylesheet, replace this with your own base-styles.css using Rawgit\n",
    "    \"https://rawgit.com/xhlulu/9a6e89f418ee40d02b637a429a876aa9/raw/f3ea10d53e33ece67eb681025cedc83870c9938d/base-styles.css\",\n",
    "    # Custom Stylesheet, replace this with your own custom-styles.css using Rawgit\n",
    "    \"https://cdn.rawgit.com/plotly/dash-svm/bb031580/custom-styles.css\"\n",
    "]\n",
    "\n",
    "for css in external_css:\n",
    "    app.css.append_css({\"external_url\": css})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T00:05:01.545903Z",
     "start_time": "2018-08-20T23:56:13.310437Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://localhost:10003\" target=\"_new\">Open in new window</a><hr>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:10003/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [21/Aug/2018 00:56:27] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Aug/2018 00:56:31] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Aug/2018 00:56:31] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Aug/2018 00:56:31] \"GET /favicon.ico HTTP/1.1\" 200 -\n",
      "C:\\tools\\Anaconda3\\lib\\site-packages\\seaborn\\categorical.py:598: FutureWarning:\n",
      "\n",
      "remove_na is deprecated and is a private function. Do not use.\n",
      "\n",
      "C:\\tools\\Anaconda3\\lib\\site-packages\\seaborn\\categorical.py:826: FutureWarning:\n",
      "\n",
      "remove_na is deprecated and is a private function. Do not use.\n",
      "\n",
      "C:\\Users\\rquintino\\AppData\\Roaming\\Python\\Python36\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning:\n",
      "\n",
      "Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "\n",
      "C:\\tools\\Anaconda3\\lib\\site-packages\\seaborn\\categorical.py:1460: FutureWarning:\n",
      "\n",
      "remove_na is deprecated and is a private function. Do not use.\n",
      "\n",
      "127.0.0.1 - - [21/Aug/2018 00:56:34] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "C:\\tools\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning:\n",
      "\n",
      "convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\n",
      "For all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "\n",
      "127.0.0.1 - - [21/Aug/2018 00:56:34] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Aug/2018 00:57:16] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Aug/2018 00:57:16] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Aug/2018 00:59:45] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Aug/2018 00:59:45] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Aug/2018 01:00:11] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Aug/2018 01:00:11] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# use <esc> i+i on Jupyter to quick interrupt & get control back to jupyter\n",
    "show_app(app=app,port=10003)\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
